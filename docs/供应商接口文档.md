---
pageClass: api-page
title: 接口
---

# Create chat completion

```
POST https://zenmux.ai/api/v1/chat/completions
```

Create chat completions 接口兼容 OpenAI 的 [Create chat completion](https://platform.openai.com/docs/api-reference/chat/create) 接口，用于对话型大语言模型推理调用。


下面列出了所有模型可能支持的参数，不同模型的支持参数有所不同，每个模型具体支持的参数请参见各模型详情页。

## Request body

### messages `array` <font color="red">必选</font>

以对话的消息列表形式输入给大模型的提示词。根据模型的能力不同，支持的消息类型也会有所不同，比如 文本、图片、音频、视频。具体支持的参数，请查看各模型生产商的文档。

messages 里的每个元素表示一条对话消息，每条消息由 role 和 content 组成，详情参见 OpenAI 定义：[messages](https://platform.openai.com/docs/api-reference/chat/create#chat_create-messages)。

### model `string` <font color="red">必选</font>

此次推理调用的模型 ID，格式为 &lt;供应商&gt;/&lt;模型名称&gt;，如 openai/gpt-5，可以从各模型的详情页获得。

### stream `boolean` <font color="gray">可选</font> `默认 false`

指定是否流式响应。只有当显式指定 `stream: true` 时，才会以 Server-Sent Event 协议进行流式响应。否则一次性返回所有生成内容。

### max_completion_tokens `integer` <font color="gray">可选</font> 

限制模型生成内容的长度，包括思考过程。如果不传，会采用模型的默认限制。各模型的最大生成内容长度可以详情页获得。

### temperature `float` <font color="gray">可选</font> `默认 1`

决定样本的 temerature，通常取值范围是 0 到 2，但模型不同，取值范围也会有所不同，比如 Claude 系列的模型取值范围是 0 到 1。值越大，生成内容的随机性就越高。

一般不建议和 top_p 一起使用。

### top_p `float` <font color="gray">可选</font> `默认 1`

截取样本的比例，取值越大，截取样本的数量就越多，生成内容的随机性就越高。

一般不建议和 temperature 一起使用。

### frequency_penalty `float` <font color="gray">可选</font> `默认 0`

取值范围 -2.0 至 2.0，是文本生成模型中用于控制重复词汇使用的参数，通过降低高频词汇的生成概率来提升文本多样性。值越大，重复性就越低。

### presence_penalty `float` <font color="gray">可选</font> `默认 0`

用于减少词汇重复的参数，通过惩罚已出现词汇的生成概率，降低其被再次选中的可能性，从而提升文本多样性。

### seed `integer` <font color="gray">可选</font>

用于控制大模型尽可能根据相同 seed 生成相同的内容。如果不传，每次都会随机使用不同的 seed。

### logit_bias `map` <font color="gray">可选</font> `默认 null`

可以用来调整模型对特定类别的偏好程度，通过增加或减少对某些类别的偏置，可以影响模型的输出结果。

使用方法参见 OpenAI 官方说明：[logit_bias](https://platform.openai.com/docs/api-reference/chat/create#chat_create-logit_bias)。

### logprobs `boolean` <font color="gray">可选</font> `默认 false`

生成过程中返回的每个 token 的概率分布信息，主要用于分析模型生成过程的置信度及调试模型。

### top_logprobs `integer` <font color="gray">可选</font>

一个介于 0 和 20 之间的整数，指定要在每个标记位置返回的最可能标记的数量，每个标记都有一个关联的对数概率。如果使用了此参数，logprobs 必须为 true。

### response_format `object` <font color="gray">可选</font>

用于控制模型输出结构化内容，如果不传默认不使用结构化输出。关于结构化输出详细使用方法参见[结构化输出](../advanced/structured-output.md)。

### stop `string/array` <font color="gray">可选</font> `默认 null`

仅部分模型支持，用于指定终止符，可以是字符串，也可以是字符串数组（指定多个）。模型的返回结果中不会包含终止符。

### tools `array` <font color="gray">可选</font>

大模型可以选择的工具列表，如果不传则不使用工具调用，当前仅支持 function 类型的工具。关于工具调用详细使用方法参见 [工具调用](../advanced/tool-calls.md)

### tool_choice `string/object` <font color="gray">可选</font>

用于控制模型如何选择使用工具，与 tools 参数搭配使用。'none' 表示告诉模型不要使用任何工具，'auto' 表示模型可以自由决定是否使用工具及使用哪几个工具，'required' 表示模型必须选择使用工具。同时也可以传 object 告诉模型必须选择使用指定的模型。

如果 tools 为空，默认为 none，如果 tools 不为空，默认为 auto。

### parallel_tool_calls `boolean` <font color="gray">可选</font> `默认 true`

控制模型是否可以一次选择多个模型。

### stream_options `object` <font color="gray">可选</font>

用于控制流式响应的返回内容，仅当 stream: true 时可用。

### reasoning `object` <font color="gray">可选</font>

用于控制 reasoning 输出，支持同时指定 effort 与 max_tokens，根据模型不同，生效的字段也不同。详情参见 [推理模型](../guide/advanced/reasoning.md)。


## Returns

如果 stream: true，会用 Server-Sent Event 协议响应，响应的每一个内容是 chat completion chunk；如果 stream: false，会响应 JSON 形式表示的 chat completion。

### Chat completion chunk

表示大模型流式响应返回的一个数据分片，当 stream: true 时，会按顺序返回很多个 chat completion chunk。

#### id `string`

表示本次生成的 generation id，全局唯一。可用于通过 [Get generation](../platform/get-generation.md) 接口查询本次生成的信息，如用量和费用等。

#### choices `array`

表示模型的输出，是一个列表，数组中最多只会存在一个元素，与 OpenAI 不同，我们不支持通过 n 来支持同时多个输出。另外，在 stream_option.include_usage: true 时，最后一个 chunk 的 choices 列表将会是空的，里面不会有元素。

choice 属性定义

##### delta `object`

表示模型输出的一个内容片段。

##### content `string`

表示模型正常输出的内容。

##### reasoning `string`

表示模型输出的推理内容。

##### tool_calls `array`

表示模型输出了工具调用。

#### finish_reason `string`

结束生成标记，如果非空表示当前是最后一个内容片段。其取值通常有 stop、length、content_filter 等。具体取值范围请参见各模型厂家官方定义。

#### index `integer`

第几个 choice，与 n 有关，因为我们不支持通过 n 来同时获得多个输出，所以只会有一个 choice，并且 index 值为 0。

#### logprobs `object`

Log probability information for the choice.

#### usage `object`

表示此次生成的用量信息。如果 stream_options.include_usage: true，则会额外输出一个 choices 为空数组的 chunk，这个 chunk 上会包含用量信息。

### Chat Completion

stream: false 时，接口返回的数据结构，会一次性返回所有模型生成的内容，包含用量等信息。

#### id `string`

表示本次生成的 generation id，全局唯一。可用于通过 [Get generation](../platform/get-generation.md) 接口查询本次生成的信息，如用量和费用等。

#### choices `array`

表示模型的输出，是一个列表，数组中最多只会存在一个元素，与 OpenAI 不同，我们不支持通过 n 来支持同时多个输出。

choice 属性定义

##### message `object`

表示模型生成的一条消息。

##### content `string`

表示模型正常输出的内容。

##### reasoning `string`

表示模型输出的推理内容。

##### tool_calls

表示模型输出了工具调用。

#### finish_reason `string`

结束生成的原因，其取值通常有 stop、length、content_filter 等。具体取值范围请参见各模型厂家官方定义。

#### index `integer`

第几个 choice，与 n 有关，因为我们不支持通过 n 来同时获得多个输出，所以只会有一个 choice，并且 index 值为 0。

#### logprobs `object`

Log probability information for the choice.
:::

#### usage `object`

表示此次生成的用量信息。



::: api-request POST /api/v1/chat/completions

```TypeScript
import OpenAI from "openai";

const openai = new OpenAI({
  baseURL: 'https://zenmux.ai/api/v1',
  apiKey: '<ZENMUX_API_KEY>',
});

async function main() {
  const completion = await openai.chat.completions.create({
    model: "openai/gpt-5", 
    messages: [
      {
        role: "user",
        content: "What is the meaning of life?", 
      },
    ],
  });

  console.log(completion.choices[0].message);
}

main();
```

```Python
from openai import OpenAI

client = OpenAI(
    base_url="https://zenmux.ai/api/v1",
    api_key="<your_ZENMUX_API_KEY>", 
)

completion = client.chat.completions.create(
    model="openai/gpt-5", 
    messages=[
        {
            "role": "user",
            "content": "What is the meaning of life?"
        }
    ]
)

print(completion.choices[0].message.content)
```

```Shell
curl https://zenmux.ai/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $ZENMUX_API_KEY" \
  -d '{
    "model": "openai/gpt-5",
    "messages": [
      {
        "role": "user",
        "content": "What is the meaning of life?"
      }
    ]
  }'
```

:::



\# 推理模型 ZenMux 在 Create chat completion 接口中提供了对模型推理行为的精细控制能力。通过 `reasoning_effort` 和 `reasoning` 两种参数配置方式,您可以根据任务复杂度灵活调整模型的推理深度和资源分配。 ## 参数说明 ### reasoning_effort 遵循 OpenAI 协议的推理强度参数,用于控制模型的推理深度。 **可选值:** - `low`: 低强度推理,适合简单任务 - `medium`: 中等强度推理(默认值) - `high`: 高强度推理,适合复杂任务 - `minimal`: 最小推理强度 ::: tip 默认行为 如果不传递该参数,系统默认使用 `medium` 级别。 ::: ### reasoning `reasoning` 参数提供了更精细的推理控制能力,支持以下字段: ```json {  "reasoning": {    "effort": "medium",    "max_tokens": 1024,    "enabled": true  } } ``` #### effort 等价于 `reasoning_effort` 参数,供仅支持 `reasoning_effort` 的模型使用。可选值与 `reasoning_effort` 相同。 #### max_tokens 用于限制推理 token 的最大长度,供支持 thinking budget 的模型使用。通过该参数可以精确控制模型在推理阶段消耗的 token 数量。 #### enabled 控制是否启用推理功能。默认值为 `true`,设置为 `false` 可关闭推理行为。 ## 参数优先级与自动计算 ZenMux 会根据您传递的参数自动计算和补充模型所需的参数,确保最佳的推理效果。 ### 默认参数补充 当 `reasoning_effort` 和 `reasoning` 都不传递时,系统自动应用以下默认配置: ```json {  "reasoning_effort": "medium",  "reasoning": {    "effort": "medium"  } } ``` ### max_tokens 自动计算 当用户指定了 `max_completion_tokens`,或模型本身具有 `max_completion_tokens` 限制时,系统会根据 `reasoning.effort` 自动计算 `reasoning.max_tokens`。 **计算规则:** ``` low:    20% 的 max_completion_tokens medium: 50% 的 max_completion_tokens high:   80% 的 max_completion_tokens ``` ### effort 反向推算 当用户传递了 `max_tokens` 但未指定 `effort` 时,系统会根据以下规则反向推算 `effort`: 1. 计算占比: `reasoning.max_tokens / max_completion_tokens` 2. 将占比与标准档位(20%, 50%, 80%)进行比较 3. 选择最接近的档位作为 `effort` 值 **示例:** 如果 `reasoning.max_tokens / max_completion_tokens = 30%`,系统将自动设置 `effort` 为 `low`。 ## 使用方式 ### OpenAI Python SDK 原生调用 使用 OpenAI Python SDK 的原生调用方式,直接传递 `reasoning_effort` 参数: ```python from openai import OpenAI import os client = OpenAI(    base_url="https://zenmux.ai/api/v1",    api_key=os.getenv("ZENMUX_API_KEY"), ) completion = client.chat.completions.create(    model="qwen/qwen3-max-preview",    reasoning_effort="high",    messages=[        {          "role": "user",          "content": "What is the meaning of life?"        }    ] ) print(completion.choices[0]) ``` ::: warning 原生调用方式的限制 使用 OpenAI SDK 的原生 `reasoning_effort` 参数时,无法精确控制推理 token 的数量。该方式仅支持推理强度级别的控制(low/medium/high),不支持 `max_tokens` 的精确设置。若需要更精细的控制,请使用高级用法。 ::: ### OpenAI Python SDK 高级用法 通过 `extra_body` 参数实现更精细的推理控制: ```python from openai import OpenAI import os client = OpenAI(    base_url="https://zenmux.ai/api/v1",    api_key=os.getenv("ZENMUX_API_KEY"), ) response = client.chat.completions.create(    model="qwen/qwen3-max-preview",    messages=[        {"role": "user", "content": "How would you build the world's tallest skyscraper?"}    ],    extra_body={        "reasoning": {            "effort": "high",            "max_tokens": 2048        }    }, ) msg = response.choices[0].message print(getattr(msg, "reasoning", None)) print(msg.content) ``` ### cURL 调用方式 #### 使用 reasoning 参数 ```bash curl https://zenmux.ai/api/v1/chat/completions \  -H "Content-Type: application/json" \  -H "Authorization: Bearer $ZENMUX_API_KEY" \  -d '{    "model": "qwen/qwen3-max-preview",    "messages": [      {        "role": "user",        "content": "What is the meaning of life?"      }    ],    "reasoning": {      "effort": "high",      "max_tokens": 1024    }  }' ``` #### 使用 reasoning_effort 参数 ```bash curl https://zenmux.ai/api/v1/chat/completions \  -H "Content-Type: application/json" \  -H "Authorization: Bearer $ZENMUX_API_KEY" \  -d '{    "model": "qwen/qwen3-max-preview",    "messages": [      {        "role": "user",        "content": "What is the meaning of life?"      }    ],    "reasoning_effort": "high"  }' ``` ### 推理结果提取 获取模型的推理过程,了解模型如何得出最终答案: ```python from openai import OpenAI import os client = OpenAI(    base_url="https://zenmux.ai/api/v1",    api_key=os.getenv("ZENMUX_API_KEY"), ) response = client.chat.completions.create(    model="qwen/qwen3-max-preview",    messages=[        {"role": "user", "content": "Solve this math problem: 2x + 5 = 15"}    ],    reasoning_effort="high", ) message = response.choices[0].message # 检查是否有推理内容 if hasattr(message, 'reasoning') and message.reasoning:    print("推理过程:")    print(message.reasoning)    print("\n最终答案:")    print(message.content) else:    print("直接回答:")    print(message.content) ``` ::: tip 最佳实践 - 对于简单的事实性问题,使用 `low` 或 `medium` 推理强度即可 - 对于需要复杂逻辑推导的任务(如数学问题、代码生成),建议使用 `high` 推理强度 - 使用 `max_tokens` 参数可以在保证推理质量的同时控制成本  :::













\# 流式 ZenMux 允许任何模型以流式的方式逐步返回生成结果，而非一次性返回完整响应。流式输出能让用户第一时间看到大模型输出的第一个 Token，减少用户的等待时间。这种方式可以显著提升用户体验，尤其适用于实时对话、长文本生成等场景。 你可以通过在请求中将 `stream` 参数设置为 `true` 的方式来使用流式输出，并获得流式输出的响应。以下是两种调用示例： ## 方法一：使用 OpenAI 兼容接口 (推荐) ::: code-group ```python [Python] from openai import OpenAI client = OpenAI(    base_url="https://zenmux.ai/api/v1",    api_key="<你的 ZENMUX_API_KEY>", # [!code highlight] ) stream = client.chat.completions.create(    model="openai/gpt-5",    messages=[        {            "role": "user",            "content": "生命的意义是什么？"         }    ],    # 通过设置 stream=True 开启流式输出模式    stream=True, # [!code highlight] ) # 当启用流式输出模式（stream=True），返回的内容会发生变化。 # 需要通过循环逐个访问返回值中每个单独的块（chunk） for chunk in stream: # [!code highlight] delta = chunk.choices[0].delta # [!code highlight] <-- 使用 delta 字段  if delta.content: 	print(delta.content, end="") ``` ```ts [TypeScript] import OpenAI from "openai"; const openai = new OpenAI({  baseURL: "https://zenmux.ai/api/v1",  apiKey: "<你的 ZENMUX_API_KEY>", // [!code highlight] }); async function main() {  const stream = await openai.chat.completions.create({    model: "openai/gpt-5",    messages: [      {        role: "user",        content: "生命的意义是什么？",      },    ],    // 通过设置 stream=True 开启流式输出模式    stream: true, // [!code highlight]  });   // 当启用流式输出模式（stream=True），返回的内容会发生变化。  // 需要通过循环逐个访问返回值中每个单独的块（chunk）  for await (chunk of stream) { // [!code highlight]     delta = chunk.choices[0].delta // [!code highlight] <-- 使用 delta 字段        if (delta.content) {        console.log(delta.content)    }  } } main(); ``` ::: --- ## 方法二：直接调用 ZenMux API ::: code-group ```python [Python (httpx)] import httpx import json async def stream_openai_chat_completion():    api_key = "<你的 ZENMUX_API_KEY>" # [!code highlight]    headers = {        "Authorization": f"Bearer {api_key}",    }    payload = {        "model": "openai/gpt-5",        "messages": [            {                "role": "user",                "content": "生命的意义是什么？"            }        ],        "stream": True # [!code highlight]    }     async with httpx.AsyncClient() as client:        async with client.stream(method="POST", url="https://zenmux.ai/api/v1/chat/completions", headers=headers, json=payload, timeout=None) as response:            response.raise_for_status()             async for chunk in response.aiter_bytes():                decoded_chunk = chunk.decode('utf-8')                print(decoded_chunk) if __name__ == "__main__":    import asyncio    asyncio.run(stream_openai_chat_completion()) ``` ```typescript [TypeScript (fetch)] fetch("https://zenmux.ai/api/v1/chat/completions", {  method: "POST",  headers: {    Authorization: "Bearer <你的 ZENMUX_API_KEY>", // [!code highlight]    "Content-Type": "application/json",  },  body: JSON.stringify({    model: "openai/gpt-5",     messages: [      {        role: "user",        content: "生命的意义是什么？",      },    ],    stream: true // [!code highlight]  }), })  .then(async (response) => {    const textDecoder = new TextDecoder();    for await (const chunk of response.body) {      const textChunk = textDecoder.decode(chunk);      console.log(textChunk)    }  }) ``` ```bash [Shell (cURL)] curl "https://zenmux.ai/api/v1/chat/completions" \  -H "Content-Type: application/json" \   -H "Authorization: Bearer $ZENMUX_API_KEY" \  -d '{      "model": "openai/gpt-5",     "messages": [       {         "role": "user",         "content": "生命的意义是什么？"       }     ],     "stream": true  }' ```